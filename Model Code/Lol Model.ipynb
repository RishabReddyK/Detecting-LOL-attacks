{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74af3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0a6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import hashlib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96e83d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start ms-appinstaller://?source=https://pasteb...</td>\n",
       "      <td>This command could potentially download and ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>start ms-appinstaller://?source=https://10.101...</td>\n",
       "      <td>This command can be used to initiate the insta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start ms-appinstaller://?source=https://11.101...</td>\n",
       "      <td>This command could be used to initiate the ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\\\Windows\\\\System32\\\\at.exe at 09:00 /intera...</td>\n",
       "      <td>This command schedules a task to run every day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATBroker.exe /start malware</td>\n",
       "      <td>Potential use: Launches malware through the AT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  start ms-appinstaller://?source=https://pasteb...   \n",
       "1  start ms-appinstaller://?source=https://10.101...   \n",
       "2  start ms-appinstaller://?source=https://11.101...   \n",
       "3  C:\\\\Windows\\\\System32\\\\at.exe at 09:00 /intera...   \n",
       "4                        ATBroker.exe /start malware   \n",
       "\n",
       "                                            response  \n",
       "0  This command could potentially download and ex...  \n",
       "1  This command can be used to initiate the insta...  \n",
       "2  This command could be used to initiate the ins...  \n",
       "3  This command schedules a task to run every day...  \n",
       "4  Potential use: Launches malware through the AT...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_data = 'cmd_huge_known_commented.csv'\n",
    "df = pd.read_csv(cmd_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0984dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 2)\n",
      "(467, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df[~df.duplicated()]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a53e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005035877227783203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 467,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_unique_id(input_string):\n",
    "    # Create a unique ID by hashing the input string and appending a random UUID\n",
    "    unique_id = hashlib.sha1(input_string.encode())\n",
    "    unique_id = unique_id.hexdigest()\n",
    "    return unique_id\n",
    "\n",
    "df['hash_id'] = df['prompt'].progress_apply(generate_unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c4e2f",
   "metadata": {},
   "source": [
    "# Labelling Malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5271b168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060541629791259766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 467,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def is_malacious(cmd):\n",
    "    if 'malicious' in cmd:\n",
    "        return True\n",
    "    if 'potential' in cmd:\n",
    "        return True\n",
    "    return False\n",
    "df['is_malicious'] = df['response'].progress_apply(is_malacious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4bcb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_malicious\n",
       "True     329\n",
       "False    138\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_malicious'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beda9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Labelled_Windows_Cmd.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9341f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import pandas as pd\n",
    "\n",
    "# # Set your OpenAI API key\n",
    "# openai.api_key = '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'\n",
    "\n",
    "# # Define a function to send a request to GPT-3\n",
    "# def classify_malicious(text):\n",
    "#     prompt = f\"Is this text malicious?\\n{text}\"\n",
    "#     response = openai.Completion.create(\n",
    "#         engine=\"text-davinci-003\",  # You can experiment with different engines\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=50,  # Adjust as needed\n",
    "#         temperature=0.7  # Adjust as needed\n",
    "#     )\n",
    "#     return response.choices[0].text.lower().strip() == 'true'\n",
    "\n",
    "# # Create a new column 'is_malicious' and populate it based on GPT-3 responses\n",
    "# temp_df = df.head(5)\n",
    "# temp_df['is_malicious'] = temp_df['response'].apply(classify_malicious)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b29d1",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8a1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X,y = df['prompt'], df['is_malicious']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa903b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X, y = df['prompt'], df['is_malicious']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Define a custom transformer using DistilBERT\n",
    "# class DistilBERTVectorizer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, max_token_length=512):\n",
    "#         self.max_token_length = max_token_length\n",
    "#         self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#         self.model = DistilBertModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         tokenized = self.tokenizer(X.tolist(), return_tensors='pt', truncation=True, padding=True, max_length=self.max_token_length)\n",
    "#         with torch.no_grad():\n",
    "#             output = self.model(**tokenized)\n",
    "#         return output['last_hidden_state'][:, 0, :].numpy()\n",
    "\n",
    "# # Build a pipeline with DistilBERTVectorizer and RandomForestClassifier\n",
    "# model = Pipeline([\n",
    "#     ('vectorizer', DistilBERTVectorizer(max_token_length=150)),\n",
    "#     ('classifier', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# classification_rep = classification_report(y_test, predictions)\n",
    "\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print('Classification Report:\\n', classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f343d75",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82901e91",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998ea4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X, y = df['prompt'], df['is_malicious']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=300)  # max_features\n",
    "\n",
    "# Transform data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2dc69",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e33e2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbf6064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00548863410949707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 467,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c0882dd4ff4b18813e22b418bac022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# xgboost_model.save_model('xgboost_model.json')\n",
    "\n",
    "def get_spacy_vectors(text):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector.tolist()\n",
    "\n",
    "df['spacy_vectors'] = df['prompt'].progress_apply(get_spacy_vectors)\n",
    "\n",
    "X, y = df['spacy_vectors'].values.tolist(), df['is_malicious']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99fa135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed860",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b773879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9302949061662198\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.80      0.87       110\n",
      "        True       0.92      0.98      0.95       263\n",
      "\n",
      "    accuracy                           0.93       373\n",
      "   macro avg       0.94      0.89      0.91       373\n",
      "weighted avg       0.93      0.93      0.93       373\n",
      "\n",
      "Test Accuracy: 0.723404255319149\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.43      0.48        28\n",
      "        True       0.78      0.85      0.81        66\n",
      "\n",
      "    accuracy                           0.72        94\n",
      "   macro avg       0.66      0.64      0.65        94\n",
      "weighted avg       0.71      0.72      0.71        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and train the RF\n",
    "classifier = RandomForestClassifier(n_estimators = 50)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "classification_rep = classification_report(y_train, y_pred)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', classification_rep)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eac2fb",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaaf77b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9302949061662198\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.79      0.87       110\n",
      "        True       0.92      0.99      0.95       263\n",
      "\n",
      "    accuracy                           0.93       373\n",
      "   macro avg       0.94      0.89      0.91       373\n",
      "weighted avg       0.93      0.93      0.93       373\n",
      "\n",
      "Accuracy: 0.7446808510638298\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.50      0.54        28\n",
      "        True       0.80      0.85      0.82        66\n",
      "\n",
      "    accuracy                           0.74        94\n",
      "   macro avg       0.69      0.67      0.68        94\n",
      "weighted avg       0.74      0.74      0.74        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and train the XGB\n",
    "classifier = XGBClassifier(n_estimators = 200)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "classification_rep = classification_report(y_train, y_pred)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', classification_rep)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3dbe9",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ab44f",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf4d04f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 100%|███████████████████████████████████████████████████████████████████████| 432/432 [01:53<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 3}\n",
      "Train Accuracy: 0.9249329758713136\n",
      "Classification Report (Train):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.77      0.86       110\n",
      "        True       0.91      0.99      0.95       263\n",
      "\n",
      "    accuracy                           0.92       373\n",
      "   macro avg       0.94      0.88      0.90       373\n",
      "weighted avg       0.93      0.92      0.92       373\n",
      "\n",
      "Test Accuracy: 0.7340425531914894\n",
      "Classification Report (Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.39      0.47        28\n",
      "        True       0.77      0.88      0.82        66\n",
      "\n",
      "    accuracy                           0.73        94\n",
      "   macro avg       0.68      0.64      0.65        94\n",
      "weighted avg       0.72      0.73      0.72        94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Build and train the RandomForestClassifier with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10, 12],\n",
    "    'min_samples_leaf': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['n_estimators']) * len(param_grid['max_depth']) * \\\n",
    "    len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])\n",
    "\n",
    "# Create a progress bar\n",
    "with tqdm(total=total_combinations, desc=\"Grid Search Progress\") as pbar:\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                    # Build and train the RandomForestClassifier\n",
    "                    classifier = RandomForestClassifier(\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        min_samples_leaf=min_samples_leaf\n",
    "                    )\n",
    "                    classifier.fit(X_train, y_train)\n",
    "\n",
    "                    # Evaluate on the validation set\n",
    "                    predictions = classifier.predict(X_test)\n",
    "                    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if accuracy > best_score:\n",
    "                        best_score = accuracy\n",
    "                        best_params = {\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf\n",
    "                        }\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_classifier = RandomForestClassifier(**best_params)\n",
    "    best_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on training set\n",
    "y_pred_train = best_classifier.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "classification_rep_train = classification_report(y_train, y_pred_train)\n",
    "\n",
    "# Evaluate the best model on test set\n",
    "y_pred_test = best_classifier.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "classification_rep_test = classification_report(y_test, y_pred_test)\n",
    "\n",
    "print(f'Best Model Parameters: {best_params}')\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_train}')\n",
    "print('Classification Report (Train):\\n', classification_rep_train)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_test}')\n",
    "print('Classification Report (Test):\\n', classification_rep_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b443512",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc344aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 100%|███████████████████████████████████████████████████████████████████████| 729/729 [03:57<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'n_estimators': 50, 'max_depth': 1, 'learning_rate': 0.3, 'subsample': 1.0, 'min_child_weight': 1}\n",
      "Train Accuracy: 0.8793565683646113\n",
      "Classification Report (Train):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.65      0.76       110\n",
      "        True       0.87      0.98      0.92       263\n",
      "\n",
      "    accuracy                           0.88       373\n",
      "   macro avg       0.90      0.81      0.84       373\n",
      "weighted avg       0.88      0.88      0.87       373\n",
      "\n",
      "Test Accuracy: 0.8085106382978723\n",
      "Classification Report (Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.50      0.61        28\n",
      "        True       0.82      0.94      0.87        66\n",
      "\n",
      "    accuracy                           0.81        94\n",
      "   macro avg       0.80      0.72      0.74        94\n",
      "weighted avg       0.80      0.81      0.79        94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Build and train the XGBClassifier with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "total_combinations = len(param_grid['n_estimators']) * len(param_grid['max_depth']) * \\\n",
    "    len(param_grid['learning_rate']) * len(param_grid['subsample']) * len(param_grid['min_child_weight'])\n",
    "\n",
    "# Create a progress bar\n",
    "with tqdm(total=total_combinations, desc=\"Grid Search Progress\") as pbar:\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                for subsample in param_grid['subsample']:\n",
    "                    for min_child_weight in param_grid['min_child_weight']:\n",
    "                        # Build and train the XGBClassifier\n",
    "                        classifier = XGBClassifier(\n",
    "                            n_estimators=n_estimators,\n",
    "                            max_depth=max_depth,\n",
    "                            learning_rate=learning_rate,\n",
    "                            subsample=subsample,\n",
    "                            min_child_weight=min_child_weight\n",
    "                        )\n",
    "                        classifier.fit(X_train, y_train)\n",
    "\n",
    "                        # Evaluate on the validation set\n",
    "                        predictions = classifier.predict(X_test)\n",
    "                        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "                        # Update the progress bar\n",
    "                        pbar.update(1)\n",
    "\n",
    "                        if accuracy > best_score:\n",
    "                            best_score = accuracy\n",
    "                            best_params = {\n",
    "                                'n_estimators': n_estimators,\n",
    "                                'max_depth': max_depth,\n",
    "                                'learning_rate': learning_rate,\n",
    "                                'subsample': subsample,\n",
    "                                'min_child_weight': min_child_weight\n",
    "                            }\n",
    "\n",
    "    # Get the best model from the grid search\n",
    "    best_classifier = XGBClassifier(**best_params)\n",
    "    best_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on training set\n",
    "y_pred_train = best_classifier.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "classification_rep_train = classification_report(y_train, y_pred_train)\n",
    "\n",
    "# Evaluate the best model on test set\n",
    "y_pred_test = best_classifier.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "classification_rep_test = classification_report(y_test, y_pred_test)\n",
    "\n",
    "print(f'Best Model Parameters: {best_params}')\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_train}')\n",
    "print('Classification Report (Train):\\n', classification_rep_train)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_test}')\n",
    "print('Classification Report (Test):\\n', classification_rep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91a9c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier.save_model('xgboost_model_11122023.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5daa47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02502a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229ee50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ab11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import spacy\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c98d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labeled data\n",
    "labeled_data = pd.read_excel('Labelled_Windows_Cmd.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b361b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>powershell</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/s</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bitsadmin</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>verclsid.exe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>{CLSID}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>wmic.exe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>\"c:\\\\ads\\\\file.txt:program.exe\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>nul)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>824 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              prompt  count\n",
       "0                         powershell     37\n",
       "1                                  &     24\n",
       "2                               call     19\n",
       "3                                 /s     18\n",
       "4                          bitsadmin     17\n",
       "..                               ...    ...\n",
       "819                     verclsid.exe      1\n",
       "820                          {CLSID}      1\n",
       "821                         wmic.exe      1\n",
       "822  \"c:\\\\ads\\\\file.txt:program.exe\"      1\n",
       "823                             nul)      1\n",
       "\n",
       "[824 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_words = pd.DataFrame(labeled_data[labeled_data['is_malicious'] == 1]['prompt'].str.split().explode().value_counts()).reset_index()\n",
    "malicious_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f746a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.bar(malicious_words.head(10), x=malicious_words.head(10).index, y='prompt', title=\"Common Words Frequency for Malicious Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7db172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97675d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
